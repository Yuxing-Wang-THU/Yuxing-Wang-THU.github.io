<hr>
<h2 align="center">Dynamics-Adaptive Continual Reinforcement Learning via Progressive Contextualization<h2></h2>

<p align="center">Tiantian Zhang, Zichuan Lin, Yuxing Wang, Deheng Ye, Qiang fu, Xueqian Wang, Xiu Li, Bo Yuan</p>

</br>
<div align="center"><img src="/assets/img/publication_preview/daco.jpg" width="500"></div>

<p style="text-align:justify;"><strong>Abstract:</strong> A key challenge of continual reinforcement learning (CRL) in dynamic environments is to promptly adapt the RL agentâ€™s behavior as the environment changes over its lifetime, while minimizing the catastrophic forgetting of the learned information. To address this challenge, in this article, we propose DaCoRL, i.e., dynamics-adaptive continual RL. DaCoRL learns a context-conditioned policy using progressive contextualization, which incrementally clusters a stream of stationary tasks in the dynamic environment into a series of contexts and opts for an expandable multihead neural network to approximate the policy. Specifically, we define a set of tasks with similar dynamics as an environmental context and formalize context inference as a procedure of online Bayesian infinite Gaussian mixture clustering on environment features, resorting to online Bayesian inference to infer the posterior distribution over contexts. Under the assumption of a Chinese restaurant process prior, this technique can accurately classify the current task as a previously seen context or instantiate a new context as needed without relying on any external indicator to signal environmental changes in advance. Furthermore, we employ an expandable multihead neural network whose output layer is synchronously expanded with the newly instantiated context, and a knowledge distillation regularization term for retaining the performance on learned tasks. As a general framework that can be coupled with various deep RL algorithms, DaCoRL features consistent superiority over existing methods in terms of the stability, overall performance and generalization ability, as verified by extensive experiments on several robot navigation and MuJoCo locomotion tasks.</p>
<p>ðŸ“‚ <a href="https://ieeexplore.ieee.org/document/10145851">Download paper here!</a><br></p>
<p><strong>Bibtex</strong><br></p>
<pre class="language-bash"><code class="language-bash code-highlight"><span class="code-line line-number" line="1">@article<span class="token punctuation">{</span>zhang2023dynamics,
</span><span class="code-line line-number" line="2">  <span class="token assign-left variable">title</span><span class="token operator">=</span><span class="token punctuation">{</span>Dynamics-Adaptive Continual Reinforcement Learning via Progressive Contextualization<span class="token punctuation">}</span>,
</span><span class="code-line line-number" line="3">  <span class="token assign-left variable">author</span><span class="token operator">=</span><span class="token punctuation">{</span>Zhang, Tiantian and Lin, Zichuan and Wang, Yuxing and Ye, Deheng and Fu, Qiang and Yang, Wei and Wang, Xueqian and Liang, Bin and Yuan, Bo and Li, Xiu<span class="token punctuation">}</span>,
</span><span class="code-line line-number" line="4">  <span class="token assign-left variable">journal</span><span class="token operator">=</span><span class="token punctuation">{</span>IEEE Transactions on Neural Networks and Learning Systems<span class="token punctuation">}</span>,
</span><span class="code-line line-number" line="5">  <span class="token assign-left variable">year</span><span class="token operator">=</span><span class="token punctuation">{</span><span class="token number">2023</span><span class="token punctuation">}</span>,
</span><span class="code-line line-number" line="6">  <span class="token assign-left variable">publisher</span><span class="token operator">=</span><span class="token punctuation">{</span>IEEE<span class="token punctuation">}</span>
</span><span class="code-line line-number" line="7"><span class="token punctuation">}</span>
</span></code></pre>