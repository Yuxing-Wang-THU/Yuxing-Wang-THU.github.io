<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd"> <html><body> <p><strong>Abstract:</strong> The integration of Reinforcement Learning (RL) and Evolutionary Algorithms (EAs) aims at simultaneously exploiting the sample efficiency as well as the diversity and robustness of the two paradigms. Recently, hybrid learning frameworks based on this principle have achieved great success in robot control tasks. However, in these methods, policies from the genetic population are evaluated via interactions with the real environments, severely restricting their applicability when such interactions are prohibitively costly. In this work, we propose Surrogate-assisted Controller (SC), a generic module that can be applied on top of existing hybrid frameworks to alleviate the computational burden of expensive fitness evaluation. At the heart of SC is a novel surrogate model based on the critic network in RL, which efficiently leverages historical interaction data generated by the population and makes it possible to estimate the fitness of individuals without environmental interactions. In addition, two model management strategies with the elite protection mechanism are introduced in SC to control the workflow, leading to a fast and stable optimization process. In the empirical studies, we combine SC with two state-of-the-art evolutionary reinforcement learning approaches to highlight its functionality and effectiveness. Experiments on six challenging continuous control benchmarks from the OpenAI Gym platform show that SC can not only significantly reduce the cost of interaction with the environment, but also bring better sample efficiency and dramatically boost the learning progress of the original hybrid framework.</p> <p>ðŸ“‚ <a href="https://www.sciencedirect.com/science/article/abs/pii/S0020025522012658" rel="external nofollow noopener" target="_blank">Download paper here!</a><br> ðŸ‘‰ <a href="https://github.com/Yuxing-Wang-THU/Surrogate-assisted-ERL" rel="external nofollow noopener" target="_blank">Code is available here!</a><br></p> <p><strong>Bibtex</strong><br></p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@article<span class="o">{</span>wang2022surrogate,
  <span class="nv">title</span><span class="o">={</span>A surrogate-assisted controller <span class="k">for </span>expensive evolutionary reinforcement learning<span class="o">}</span>,
  <span class="nv">author</span><span class="o">={</span>Wang, Yuxing and Zhang, Tiantian and Chang, Yongzhe and Wang, Xueqian and Liang, Bin and Yuan, Bo<span class="o">}</span>,
  <span class="nv">journal</span><span class="o">={</span>Information Sciences<span class="o">}</span>,
  <span class="nv">volume</span><span class="o">={</span>616<span class="o">}</span>,
  <span class="nv">pages</span><span class="o">={</span>539--557<span class="o">}</span>,
  <span class="nv">year</span><span class="o">={</span>2022<span class="o">}</span>,
  <span class="nv">publisher</span><span class="o">={</span>Elsevier<span class="o">}</span>
<span class="o">}</span>
</code></pre></div></div> <p>ðŸŽ¦<strong>Video</strong></p> <p>A HalfCheetah agent trained by SPDERL-I with average performance of 14000 points over 50 test seeds. The agent is able to adjust its posture more appropriately and run faster.</p> <video id="video0" controls="" preload="none" width="510"> <source id="0mp4" src="/images/2.HalfCheetah_trained_by_SPDERL-I_14000.mp4" type="video/mp4"></source> &lt;/videos&gt; </video> </body></html>